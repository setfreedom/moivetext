#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å½±è§†è§£è¯´é¡¹ç›® - ç¬¬äºŒæ­¥ï¼šå¤šæ¨¡æ€å†…å®¹ç†è§£ï¼ˆASR + è§†è§‰æè¿°ï¼‰
è¾“å…¥ï¼šoutput_step1/scenes.json + audio/ + frames/
è¾“å‡ºï¼šoutput_step2/scenes_enhanced.json
"""

import os
import json
import torch
from PIL import Image
from faster_whisper import WhisperModel
from transformers import BlipProcessor, BlipForConditionalGeneration

# ========================
# é…ç½®
# ========================
INPUT_META = "output_step1/scenes.json"
OUTPUT_DIR = "output_step2"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# è®¾å¤‡è‡ªåŠ¨é€‰æ‹©
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"

print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device} ({compute_type})")

# ========================
# åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼Œé¿å…æ— æ–‡ä»¶æ—¶æŠ¥é”™ï¼‰
# ========================
print("æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ (large-v3)...")
whisper_model = WhisperModel(
    r"G:\models\faster-whisper-large-v3",  # ğŸ‘ˆ æœ¬åœ°è·¯å¾„
    device=device,
    compute_type=compute_type,
    local_files_only=True  # å¼ºåˆ¶ç¦»çº¿
)

print("æ­£åœ¨åŠ è½½ BLIP è§†è§‰æè¿°æ¨¡å‹...")
blip_processor = BlipProcessor.from_pretrained(
    r"G:\models\blip-image-captioning-large",
    local_files_only=True
)
blip_model = BlipForConditionalGeneration.from_pretrained(
    r"G:\models\blip-image-captioning-large",
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    local_files_only=True
).to(device)

# ========================
# å·¥å…·å‡½æ•°
# ========================

def transcribe_audio(audio_path):
    """ä½¿ç”¨ Whisper è½¬å½•éŸ³é¢‘ï¼ˆä¸­æ–‡ä¼˜åŒ–ï¼‰"""
    try:
        segments, _ = whisper_model.transcribe(
            audio_path,
            language="zh",          # å¼ºåˆ¶ä¸­æ–‡
            beam_size=5,
            vad_filter=True,        # å¯ç”¨è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆå»é™éŸ³ï¼‰
            temperature=0.0         # ç¡®å®šæ€§è¾“å‡º
        )
        text = "".join([seg.text for seg in segments]).strip()
        return text if text else ""
    except Exception as e:
        print(f"âš ï¸ ASR å¤±è´¥ ({audio_path}): {e}")
        return ""

def generate_caption(image_path):
    """ä½¿ç”¨ BLIP ç”Ÿæˆå›¾åƒæè¿°"""
    try:
        image = Image.open(image_path).convert("RGB")
        inputs = blip_processor(images=image, return_tensors="pt").to(device)
        if device == "cuda":
            inputs = {k: v.half() for k, v in inputs.items()}
        
        with torch.no_grad():
            output = blip_model.generate(**inputs, max_length=50, num_beams=5)
        
        caption = blip_processor.decode(output[0], skip_special_tokens=True)
        return caption.strip()
    except Exception as e:
        print(f"âš ï¸ è§†è§‰æè¿°å¤±è´¥ ({image_path}): {e}")
        return ""

# ========================
# ä¸»æµç¨‹
# ========================

def main():
    if not os.path.exists(INPUT_META):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°ç¬¬ä¸€æ­¥è¾“å‡º: {INPUT_META}")

    with open(INPUT_META, "r", encoding="utf-8") as f:
        scenes = json.load(f)

    print(f"å…±åŠ è½½ {len(scenes)} ä¸ªåœºæ™¯ï¼Œå¼€å§‹å¤šæ¨¡æ€ç†è§£...")

    enhanced_scenes = []
    for i, scene in enumerate(scenes):
        print(f"[{i+1}/{len(scenes)}] å¤„ç†åœºæ™¯ {scene['scene_id']}...")

        # 1. ASR è½¬å½•
        asr_text = ""
        if os.path.exists(scene["audio_path"]):
            asr_text = transcribe_audio(scene["audio_path"])

        # 2. è§†è§‰æè¿°
        vision_caption = ""
        if os.path.exists(scene["frame_path"]):
            vision_caption = generate_caption(scene["frame_path"])

        # 3. èåˆä¸Šä¸‹æ–‡ï¼ˆç®€å•æ‹¼æ¥ï¼Œåç»­å¯ä¼˜åŒ–ï¼‰
        combined = []
        if vision_caption:
            combined.append(vision_caption)
        if asr_text:
            combined.append(asr_text)
        combined_context = "ã€‚".join(combined) + ("ã€‚" if combined else "")

        # 4. ä¿å­˜
        enhanced_scenes.append({
            "scene_id": scene["scene_id"],
            "start_time": scene["start_time"],
            "end_time": scene["end_time"],
            "duration": scene["duration"],
            "asr_text": asr_text,
            "vision_caption": vision_caption,
            "combined_context": combined_context
        })

    # ä¿å­˜ç»“æœ
    output_path = os.path.join(OUTPUT_DIR, "scenes_enhanced.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(enhanced_scenes, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ç¬¬äºŒæ­¥å®Œæˆï¼ç»“æœä¿å­˜è‡³: {output_path}")
    print(f"   - ç¤ºä¾‹ ASR: {enhanced_scenes[0]['asr_text'][:50]}...")
    print(f"   - ç¤ºä¾‹è§†è§‰: {enhanced_scenes[0]['vision_caption']}")

if __name__ == "__main__":
    main()